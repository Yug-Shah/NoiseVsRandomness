{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit qiskit_machine_learning pylatexenc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKI8PlL0bnW7",
        "outputId": "fad154a9-a978-49ac-e96d-b767aee87f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qiskit in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: qiskit_machine_learning in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.10/dist-packages (2.10)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (0.14.2)\n",
            "Requirement already satisfied: numpy<2,>=1.17 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.11.4)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit) (1.12)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.10/dist-packages (from qiskit) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (2.8.2)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from qiskit) (5.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qiskit) (4.11.0)\n",
            "Requirement already satisfied: symengine>=0.11 in /usr/local/lib/python3.10/dist-packages (from qiskit) (0.11.0)\n",
            "Requirement already satisfied: qiskit-algorithms>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from qiskit_machine_learning) (0.3.0)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from qiskit_machine_learning) (5.9.5)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from qiskit_machine_learning) (1.2.2)\n",
            "Requirement already satisfied: fastdtw in /usr/local/lib/python3.10/dist-packages (from qiskit_machine_learning) (0.3.4)\n",
            "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/lib/python3.10/dist-packages (from qiskit_machine_learning) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->qiskit_machine_learning) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->qiskit_machine_learning) (3.4.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=3.0.0->qiskit) (6.0.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrsjlQTvAbCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447c8c90-fda5-4368-9aff-221167b8d025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wgVYr5-TXm5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/AI_2qubits_training_data.txt'\n",
        "\n",
        "# Read the data from the file\n",
        "data = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        if line.strip():\n",
        "            binary_number, label = line.strip().split()\n",
        "            data.append((binary_number, int(label)))\n",
        "\n",
        "# Convert the data into a DataFrame\n",
        "df = pd.DataFrame(data, columns=['binary_number', 'label'])\n",
        "\n",
        "# Get the number of rows and columns in the DataFrame\n",
        "num_rows, num_columns = df.shape\n",
        "\n",
        "print(\"Number of rows:\", num_rows)\n",
        "print(\"Number of columns:\", num_columns)\n",
        "\n",
        "# Preprocess the binary_number column to convert each bit to a separate feature column\n",
        "df_features = pd.DataFrame(df['binary_number'].apply(list).tolist())\n",
        "df = pd.concat([df.drop(columns='binary_number'), df_features], axis=1)\n",
        "\n",
        "# Split the data into features (X) and labels (y)\n",
        "X = df.drop(columns='label').values\n",
        "y = df['label'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MxSPTrNXsp6",
        "outputId": "036b9ed0-6d4f-4d91-e587-04ec579e2d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 6000\n",
            "Number of columns: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "id": "otG5lKzMzwcw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92057422-eb01-4bf8-a896-ce8e24ed7281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      label  0  1  2  3  4  5  6  7  8  ... 90 91 92 93 94 95 96 97 98 99\n",
            "0         1  0  1  0  0  1  1  1  1  1  ...  1  1  1  1  1  1  0  0  1  0\n",
            "1         1  0  1  1  0  0  1  1  0  1  ...  0  1  1  0  0  0  1  1  0  1\n",
            "2         1  1  1  1  0  1  0  0  1  0  ...  0  1  1  0  0  0  0  0  1  1\n",
            "3         1  1  1  0  1  0  0  0  0  1  ...  1  1  0  1  1  1  1  1  0  1\n",
            "4         1  0  0  0  0  0  0  0  0  0  ...  0  0  1  1  1  1  0  0  1  1\n",
            "...     ... .. .. .. .. .. .. .. .. ..  ... .. .. .. .. .. .. .. .. .. ..\n",
            "5995      3  1  0  0  0  1  0  1  0  0  ...  1  1  1  1  0  0  0  0  1  0\n",
            "5996      3  1  0  0  1  0  0  1  1  1  ...  1  1  1  0  0  1  0  0  1  0\n",
            "5997      3  0  0  0  0  0  1  0  0  0  ...  0  1  0  0  1  1  1  0  1  0\n",
            "5998      3  1  1  1  0  1  0  0  1  1  ...  1  1  1  0  0  0  1  0  0  0\n",
            "5999      3  0  1  1  0  0  1  0  1  1  ...  1  1  1  1  0  0  1  1  0  0\n",
            "\n",
            "[6000 rows x 101 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_to_bits(binary_list):\n",
        "    return [[[int(i) for i in str(bit)] for bit in binary_string][0] for binary_string in binary_list]\n",
        "X = np.array(binary_to_bits(np.loadtxt('/content/drive/MyDrive/AI_2qubits_training_data.txt', dtype = str)[:,:-1]))\n",
        "Y = np.loadtxt('/content/drive/MyDrive/AI_2qubits_training_data.txt', dtype = str)[:,-1].astype(int)\n",
        "Y-=1\n",
        "def one_hot(y, num_classes):\n",
        "    return np.eye(num_classes)[y]\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_data_one_hot = one_hot(Y, 3)"
      ],
      "metadata": {
        "id": "IQtDhfXRDnTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sekcja"
      ],
      "metadata": {
        "id": "WMRxSdTiNdSu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Brik04cEFwK"
      },
      "source": [
        "## qSVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQm1VbnVEF3K"
      },
      "source": [
        "# Nowa sekcja"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## qGAN"
      ],
      "metadata": {
        "id": "yfQXXHOkbgq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit import QuantumCircuit\n",
        "from qiskit.circuit.library import RealAmplitudes\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.connectors import TorchConnector\n",
        "from torch import nn\n",
        "\n",
        "# Define the quantum generator circuit\n",
        "num_qubits = 100  # Number of qubits for the binary features\n",
        "qc = QuantumCircuit(num_qubits)\n",
        "qc.h(range(num_qubits))  # Apply Hadamard to get into a superposition\n",
        "ansatz = RealAmplitudes(num_qubits, reps=5)\n",
        "qc.compose(ansatz, inplace=True)"
      ],
      "metadata": {
        "id": "BAOLaoCxcPnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qc.decompose().draw(output=\"mpl\", style=\"clifford\")"
      ],
      "metadata": {
        "id": "Mj9HVcuwcnow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qc.num_parameters"
      ],
      "metadata": {
        "id": "3hT98mOkdK_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit.primitives import Sampler\n",
        "from qiskit_algorithms.utils import algorithm_globals\n",
        "\n",
        "shots = 10000\n",
        "sampler = Sampler(options={\"shots\": shots, \"seed\": algorithm_globals.random_seed})"
      ],
      "metadata": {
        "id": "D1hqzwDxdLux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "features = MinMaxScaler(feature_range=(0, np.pi)).fit_transform(df_features)\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(\n",
        "    X[:,:25], y_data_one_hot[:,0], train_size=15, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "uMOI5FTN_93V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of qubits is equal to the number of features\n",
        "num_qubits = 25\n",
        "\n",
        "# number of steps performed during the training procedure\n",
        "tau = 10\n",
        "\n",
        "# regularization parameter\n",
        "C = 100"
      ],
      "metadata": {
        "id": "qXgbbB9-APix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit.circuit.library import ZFeatureMap\n",
        "from qiskit_algorithms.utils import algorithm_globals\n",
        "\n",
        "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
        "\n",
        "algorithm_globals.random_seed = 12345\n",
        "\n",
        "feature_map = ZFeatureMap(feature_dimension=num_qubits, reps=1)\n",
        "\n",
        "qkernel = FidelityQuantumKernel(feature_map=feature_map)"
      ],
      "metadata": {
        "id": "r3JDyxvBAVys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit_machine_learning.algorithms import PegasosQSVC\n",
        "\n",
        "pegasos_qsvc = PegasosQSVC(quantum_kernel=qkernel, C=C, num_steps=tau)\n",
        "\n",
        "# training\n",
        "pegasos_qsvc.fit(train_features, train_labels)\n",
        "\n",
        "# testing\n",
        "pegasos_score = pegasos_qsvc.score(test_features, test_labels)\n",
        "print(f\"PegasosQSVC classification test score: {pegasos_score}\")\n",
        "print(train_labels)"
      ],
      "metadata": {
        "id": "suLpvpodAZiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit_machine_learning.connectors import TorchConnector\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "\n",
        "\n",
        "def create_generator() -> TorchConnector:\n",
        "    qnn = SamplerQNN(\n",
        "        circuit=qc,\n",
        "        sampler=sampler,\n",
        "        input_params=[],\n",
        "        weight_params=qc.parameters,\n",
        "        sparse=False,\n",
        "    )\n",
        "\n",
        "    initial_weights = algorithm_globals.random.random(qc.num_parameters)\n",
        "    return TorchConnector(qnn, initial_weights)"
      ],
      "metadata": {
        "id": "yF6Wg-XVdXlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.linear_input = nn.Linear(input_size, 20)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "        self.linear20 = nn.Linear(20, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.linear_input(input)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.linear20(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QuynxvEhdam8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = create_generator()\n",
        "discriminator = Discriminator(num_dim)"
      ],
      "metadata": {
        "id": "hYUDteu5cTJ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "cdeabb95-6619-47d4-e40d-5a87549642c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_dim' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-beb6fed7d769>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'num_dim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adversarial_loss(input, target, w):\n",
        "    bce_loss = target * torch.log(input) + (1 - target) * torch.log(1 - input)\n",
        "    weighted_loss = w * bce_loss\n",
        "    total_loss = -torch.sum(weighted_loss)\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "T95EJ0J_d5yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "lr = 0.01  # learning rate\n",
        "b1 = 0.7  # first momentum parameter\n",
        "b2 = 0.999  # second momentum parameter\n",
        "\n",
        "generator_optimizer = Adam(generator.parameters(), lr=lr, betas=(b1, b2), weight_decay=0.005)\n",
        "discriminator_optimizer = Adam(\n",
        "    discriminator.parameters(), lr=lr, betas=(b1, b2), weight_decay=0.005\n",
        ")"
      ],
      "metadata": {
        "id": "-jkYTkbhblZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def plot_training_progress():\n",
        "    # we don't plot if we don't have enough data\n",
        "    if len(generator_loss_values) < 2:\n",
        "        return\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n",
        "\n",
        "    # Generator Loss\n",
        "    ax1.set_title(\"Loss\")\n",
        "    ax1.plot(generator_loss_values, label=\"generator loss\", color=\"royalblue\")\n",
        "    ax1.plot(discriminator_loss_values, label=\"discriminator loss\", color=\"magenta\")\n",
        "    ax1.legend(loc=\"best\")\n",
        "    ax1.set_xlabel(\"Iteration\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.grid()\n",
        "\n",
        "    # Relative Entropy\n",
        "    ax2.set_title(\"Relative entropy\")\n",
        "    ax2.plot(entropy_values)\n",
        "    ax2.set_xlabel(\"Iteration\")\n",
        "    ax2.set_ylabel(\"Relative entropy\")\n",
        "    ax2.grid()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "eu1b1rH-d9_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from scipy.stats import multivariate_normal, entropy\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "num_qnn_outputs = num_discrete_values**num_dim\n",
        "\n",
        "generator_loss_values = []\n",
        "discriminator_loss_values = []\n",
        "entropy_values = []\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    valid = torch.ones(num_qnn_outputs, 1, dtype=torch.float)\n",
        "    fake = torch.zeros(num_qnn_outputs, 1, dtype=torch.float)\n",
        "\n",
        "    # Configure input\n",
        "    real_dist = torch.tensor(prob_data, dtype=torch.float).reshape(-1, 1)\n",
        "\n",
        "    # Configure samples\n",
        "    samples = torch.tensor(grid_elements, dtype=torch.float)\n",
        "    disc_value = discriminator(samples)\n",
        "\n",
        "    # Generate data\n",
        "    gen_dist = generator(torch.tensor([])).reshape(-1, 1)\n",
        "\n",
        "    # Train generator\n",
        "    generator_optimizer.zero_grad()\n",
        "    generator_loss = adversarial_loss(disc_value, valid, gen_dist)\n",
        "\n",
        "    # store for plotting\n",
        "    generator_loss_values.append(generator_loss.detach().item())\n",
        "\n",
        "    generator_loss.backward(retain_graph=True)\n",
        "    generator_optimizer.step()\n",
        "\n",
        "    # Train Discriminator\n",
        "    discriminator_optimizer.zero_grad()\n",
        "\n",
        "    real_loss = adversarial_loss(disc_value, valid, real_dist)\n",
        "    fake_loss = adversarial_loss(disc_value, fake, gen_dist.detach())\n",
        "    discriminator_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "    # Store for plotting\n",
        "    discriminator_loss_values.append(discriminator_loss.detach().item())\n",
        "\n",
        "    discriminator_loss.backward()\n",
        "    discriminator_optimizer.step()\n",
        "\n",
        "    entropy_value = entropy(gen_dist.detach().squeeze().numpy(), prob_data)\n",
        "    entropy_values.append(entropy_value)\n",
        "\n",
        "    plot_training_progress()\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f\"Fit in {elapsed:0.2f} sec\")"
      ],
      "metadata": {
        "id": "NIe0IjIheAl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from scipy.stats import entropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is the DataFrame with the given dataset\n",
        "# Split the data into features (X) and labels (y)\n",
        "X = df.drop(columns='label').values\n",
        "y = df['label'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train)\n",
        "# Convert the training data into PyTorch tensors\n",
        "X_train_tensor = X_train\n",
        "y_train_tensor = y_train\n",
        "\n",
        "# Define the number of epochs\n",
        "n_epochs = 50\n",
        "\n",
        "# Initialize lists to store loss values and entropy values for plotting\n",
        "generator_loss_values = []\n",
        "discriminator_loss_values = []\n",
        "entropy_values = []\n",
        "\n",
        "# Start the training timer\n",
        "start = time.time()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    # Train the generator and discriminator with the training data\n",
        "    for i in range(len(X_train_tensor)):\n",
        "        # Get the current training example\n",
        "        current_x = X_train_tensor[i].reshape(1, -1)\n",
        "        current_y = y_train_tensor[i]\n",
        "\n",
        "        # Generate fake data using the generator\n",
        "        gen_dist = generator(current_x)\n",
        "\n",
        "        # Train the generator\n",
        "        generator_optimizer.zero_grad()\n",
        "        generator_loss = adversarial_loss(discriminator(gen_dist), current_y)\n",
        "        generator_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "        generator_loss_values.append(generator_loss.item())\n",
        "\n",
        "        # Train the discriminator on real data\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        real_loss = adversarial_loss(discriminator(current_x), current_y)\n",
        "        real_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # Train the discriminator on fake data\n",
        "        fake_loss = adversarial_loss(discriminator(gen_dist.detach()), torch.zeros_like(current_y))\n",
        "        fake_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # Calculate the discriminator loss as the average of real and fake losses\n",
        "        discriminator_loss = (real_loss.item() + fake_loss.item()) / 2\n",
        "        discriminator_loss_values.append(discriminator_loss)\n",
        "\n",
        "        # Calculate and store the entropy value\n",
        "        entropy_value = entropy(gen_dist.detach().squeeze().numpy(), current_x.numpy())\n",
        "        entropy_values.append(entropy_value)\n",
        "\n",
        "    # Optionally, plot the training progress\n",
        "    plot_training_progress()\n",
        "\n",
        "# Stop the training timer\n",
        "elapsed = time.time() - start\n",
        "print(f\"Fit in {elapsed:0.2f} sec\")"
      ],
      "metadata": {
        "id": "uGlxFbBYfOLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Podsekcja"
      ],
      "metadata": {
        "id": "Z1uEBbqeN4cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SEKCJA"
      ],
      "metadata": {
        "id": "_G5Lo28GN1I8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "mxDS9VFjAqvk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "937q3D9VAzlM"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Perform hyperparameter tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'degree': [2, 3, 4],\n",
        "}\n",
        "\n",
        "svm_model = SVC(random_state=42)\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Perform hyperparameter tuning with RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "}\n",
        "\n",
        "svm_model = SVC(random_state=42)\n",
        "random_search = RandomizedSearchCV(svm_model, param_distributions=param_dist, n_iter=5, cv=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "NMKS-cKqbkL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "NDFlld25Xze5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n"
      ],
      "metadata": {
        "id": "QFkkQCBxXlWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],          # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30],         # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],           # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['auto', 'sqrt'],       # Number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_rf = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
        "print(\"Best Hyperparameters:\", best_params)\n"
      ],
      "metadata": {
        "id": "KULNfXpncJZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting"
      ],
      "metadata": {
        "id": "8DfNGSyaX3kV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBUNIc9wAzvF"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Create the Gradient Boosting classifier\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the Gradient Boosting model\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_gb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch"
      ],
      "metadata": {
        "id": "Q1L945YBW8hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Create the Gradient Boosting classifier\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Perform Grid Search with cross-validation (cv=5) to find the best hyperparameters\n",
        "grid_search = GridSearchCV(gb_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_gb = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the Gradient Boosting model with the best hyperparameters\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_gb)\n"
      ],
      "metadata": {
        "id": "0iD-fcbeW6NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "c6rTZWuztjNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Create the XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Map classes to [0, 1, 2]\n",
        "y_train_mapped = y_train - 1  # This will change classes [1, 2, 3] to [0, 1, 2]\n",
        "\n",
        "# Continue with the Grid Search\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train_mapped)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best XGBoost model\n",
        "y_pred_xgb = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the XGBoost model with the best hyperparameters\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(\"Best Hyperparameters for XGBoost:\", best_params)\n",
        "print(\"XGBoost Accuracy:\", accuracy_xgb)\n"
      ],
      "metadata": {
        "id": "i4qdLReQtmZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h_stlSoN3x3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CatBoost"
      ],
      "metadata": {
        "id": "BVKGPCOT3x9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "# Create the LightGBM classifier\n",
        "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Perform Grid Search with cross-validation (cv=5) to find the best hyperparameters\n",
        "grid_search = GridSearchCV(lgb_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_lgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best LightGBM model\n",
        "y_pred_lgb = best_lgb_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the LightGBM model with the best hyperparameters\n",
        "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
        "print(\"Best Hyperparameters for LightGBM:\", best_params)\n",
        "print(\"LightGBM Accuracy:\", accuracy_lgb)\n"
      ],
      "metadata": {
        "id": "QXVAMJrx3xLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "z3RxycX8X8wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming you have already defined X_train, X_test, y_train, and y_test\n",
        "\n",
        "# Convert binary numbers to integer labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_integer = label_encoder.fit_transform(y_train)\n",
        "y_test_integer = label_encoder.transform(y_test)\n",
        "\n",
        "# Check unique values in y_train_integer and y_test_integer\n",
        "print(\"Unique values in y_train:\", np.unique(y_train_integer))\n",
        "print(\"Unique values in y_test:\", np.unique(y_test_integer))\n",
        "\n",
        "print(\"Shape of y_train_integer:\", y_train_integer.shape)\n",
        "print(\"Shape of y_test_integer:\", y_test_integer.shape)\n",
        "\n",
        "# Manually split the data into training and validation sets\n",
        "X_train, X_val, y_train_integer, y_val_integer = train_test_split(X_train, y_train_integer, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Neural Network model\n",
        "nn_model = Sequential()\n",
        "nn_model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "nn_model.add(Dense(32, activation='relu'))\n",
        "nn_model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "nn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "nn_model.fit(X_train, y_train_integer, epochs=50, batch_size=32, validation_data=(X_val, y_val_integer), verbose=0)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_probabilities = nn_model.predict(X_test)\n",
        "y_pred_nn = np.argmax(y_pred_probabilities, axis=-1)\n",
        "\n",
        "# Calculate the accuracy of the Neural Network model\n",
        "accuracy_nn = accuracy_score(y_test_integer, y_pred_nn)\n",
        "print(\"Neural Network Accuracy:\", accuracy_nn)\n"
      ],
      "metadata": {
        "id": "htyjktKSX-cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "qGXQrE8d9d0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ... (Previous code for reading and preprocessing the data)\n",
        "\n",
        "# Convert the data into numerical format\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "# Reshape the input data for LSTM\n",
        "time_steps = 1  # Each sample is treated as a single time step\n",
        "X_train_lstm = X_train.reshape(X_train.shape[0], time_steps, X_train.shape[1])\n",
        "X_test_lstm = X_test.reshape(X_test.shape[0], time_steps, X_test.shape[1])\n",
        "\n",
        "# Create the LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(64, input_shape=(time_steps, X_train.shape[1])))\n",
        "lstm_model.add(Dense(32, activation='relu'))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
        "y_pred_lstm = np.round(y_pred_lstm).astype(int).flatten()  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate the accuracy of the LSTM model\n",
        "accuracy_lstm = accuracy_score(y_test, y_pred_lstm)\n",
        "print(\"LSTM Accuracy:\", accuracy_lstm)\n"
      ],
      "metadata": {
        "id": "2sE0n_hlbMAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yfQXXHOkbgq3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}